We utilize RoentGen, a model based on Stable Diffusion, to generate synthetic images of pneumonia. This approach allows us to produce a large and diverse set of medically relevant images by leveraging the power of generative models. During the image generation process, we incorporate Daam, a tool designed to capture attention maps. Daam helps us understand which regions of the input data the model focuses on while generating the images, giving us valuable insights into the inner workings of the model and enhancing the interpretability of the results.

Once we have our synthetic dataset, we combine it with real-world medical data from the QaTa-COV19 dataset. This dataset contains chest X-rays along with binary masks that identify regions of infection, which are marked as either 0 (non-infected) or 1 (infected). By merging the generated data with this real dataset, we create a more robust and comprehensive dataset, which improves the performance and generalization of our model.

We then train a nnUNet, a neural network architecture commonly used for medical image segmentation, on the combined dataset. The training process spans 200 epochs, allowing the model to learn complex patterns and features across both the synthetic and real datasets. By incorporating both types of data, we aim to enhance the model’s ability to generate accurate segmentation masks, which are essential for identifying infected areas in chest X-rays.

Finally, we apply the trained nnUNet to generate masks for two large and widely-used medical imaging datasets: NIHCXR and CheXpert. These datasets serve as the final benchmark for evaluating the model’s performance, allowing us to assess its accuracy in identifying pneumonia and its generalizability across different patient populations and imaging conditions.